---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

```{r}
library(tidyverse)
library(tidymodels)
library(janitor)
library(skimr)
library(vip)
library(parallel)
library(doParallel)
library(embed)
library(textrecipes)
library(stringr)
library(tidytext)
options(yardstick.event_first = FALSE)
library(xgboost)
```

```{r}
job <- read_csv("./job_training.csv") %>%
  clean_names() 

job %>%
  skim()
```


## Baseline Target

```{r}
job %>%
  count(fraudulent) %>%
  mutate(pct = n/sum(n)) -> job_label
job_label

job_label %>%
  ggplot(aes(x=fraudulent, y=pct)) +
  geom_col() +
  geom_text(aes(label=pct) ,color="Orange") + 
  labs(title="fraudulent Rate")

job %>%
  count(salary_range) %>%
  mutate(pct = n/sum(n)) -> salary_label
salary_label 
```

```{r}
head(job)

```
# Numeric variables
```{r}
job %>%
  ggplot(aes(job_id, fill=factor(fraudulent)))+
  geom_histogram(bins=500, position="fill") +
  labs(title = "Realationship b/w Job Id & Fraudulent") +
  ylab("pct")+
  xlab("Job Id")

job %>%
  ggplot(aes(telecommuting, fill=factor(fraudulent)))+
  geom_histogram(bins=5, position="fill") +
  labs(title = "Realationship b/w Telecommuting & Fraudulent") +
  ylab("pct")+
  xlab("Telecommuting count")

job %>%
  ggplot(aes(has_company_logo, fill=factor(fraudulent)))+
  geom_histogram(bins=5, position="fill") +
  labs(title = "Realationship b/w Company Logo & Fraudulent") +
  ylab("pct")+
  xlab("Whether having Company Logo")

job %>%
  ggplot(aes(has_questions, fill=factor(fraudulent)))+
  geom_histogram(bins=5, position="fill") +
  labs(title = "Realationship b/w Having questions & Fraudulent") +
  ylab("pct")+
  xlab("Whether have questions")
```
#histogram for numeric variables

```{r}
options(scipen = 99)

nrows <- nrow(job)
sprintf("rice rule bins = %d", floor((nrows^(1/3))*2))

lista <- c("telecommuting","has_questions","job_id","has_company_logo")

functiona <- function(data, fraudulent) {
      data %>%
        ggplot(aes(.data[[fraudulent]])) + 
            geom_histogram(aes(y = ..density..), bins = 46) + 
            stat_function(fun = dnorm, colour = "red",
              args = list(mean = mean(data[[fraudulent]], na.rm = TRUE),
                        sd = sd(data[[fraudulent]], na.rm = TRUE)))+
  labs(title = "Variable Distrbution")


}
map(lista, ~ functiona(job, .x))
```

## Categorical Data 
```{r}
#1
job %>% 
  group_by(fraudulent) %>%
  count(required_education) %>%
  pivot_wider(id_cols=required_education, values_from = n, names_from=fraudulent) %>%
  mutate(pct_1 = `1`/(`0`+`1`),
         pct_0 = 1 - pct_1) %>%
  arrange(desc(pct_1)) -> pivot_table 

pivot_table

job %>% 
  group_by(fraudulent) %>%
  count(required_education) %>%
  ggplot(aes(x=required_education,y=n, fill=fraudulent))+
  geom_col(position="fill") +
  #geom_text(aes(label=n) ,color="red") + 
  labs(title="required_education & fraudulent",x="grade",y="pct")+
  coord_flip()

#2
job %>% 
  group_by(fraudulent) %>%
  count(required_experience) %>%
  pivot_wider(id_cols=required_experience, values_from = n, names_from=fraudulent) %>%
  mutate(pct_1 = `1`/(`0`+`1`),
         pct_0 = 1 - pct_1) %>%
  arrange(desc(pct_1)) -> pivot_table 

pivot_table

job %>% 
  group_by(fraudulent) %>%
  count(required_experience) %>%
  ggplot(aes(x=required_experience,y=n, fill=fraudulent))+
  geom_col(position="fill") +
  #geom_text(aes(label=n) ,color="red") + 
  labs(title="required_experience & fraudulent",x="grade",y="pct")+
  coord_flip()

#3
job %>% 
  group_by(fraudulent) %>%
  count(employment_type) %>%
  pivot_wider(id_cols=employment_type, values_from = n, names_from=fraudulent) %>%
  mutate(pct_1 = `1`/(`0`+`1`),
         pct_0 = 1 - pct_1) %>%
  arrange(desc(pct_1)) -> pivot_table 

pivot_table

job %>% 
  group_by(fraudulent) %>%
  count(employment_type) %>%
  ggplot(aes(x=employment_type,y=n, fill=fraudulent))+
  geom_col(position="fill") +
  #geom_text(aes(label=n) ,color="red") + 
  labs(title="employment_type & fraudulent",x="grade",y="pct")+
  coord_flip()


#3
job %>% 
  group_by(fraudulent) %>%
  count(salary_range) %>%
  pivot_wider(id_cols=salary_range, values_from = n, names_from=fraudulent) %>%
  mutate(pct_1 = `1`/(`0`+`1`),
         pct_0 = 1 - pct_1) %>%
  arrange(desc(pct_1)) -> pivot_table 

pivot_table

job %>% 
  group_by(fraudulent) %>%
  count(salary_range) %>%
  ggplot(aes(x=salary_range,y=n, fill=fraudulent))+
  geom_col(bins=500, position="fill") +
  labs(title="salary_range & fraudulent",x="grade",y="pct")+
  coord_flip()
```

# coorelation
```{r}
cor_job <- job %>%
  na.omit() %>%
  select(telecommuting,has_questions,job_id,has_company_logo) %>%
  cor() %>%
  as.data.frame() %>%
  rownames_to_column(var="variable")

cor_job %>%
  pivot_longer(cols= c("telecommuting","has_questions","job_id","has_company_logo"), 
               names_to="name", 
               values_to="correlation" ) %>%
  ggplot(aes(x=variable, y=name, fill=correlation)) +
  geom_tile() +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5)) +
  scale_fill_gradient2(mid="#FBFEF9",low="#0C6291",high="#A63446")+
  geom_text(aes(label=round(correlation,4)), color="white") +
  labs(title = "Correlation Matrix of Charactive Variables", 
       x = "Variable",
       y = "Variable")
```
# Distrubution of variables
```{r}
#install.packages("ISLR")
library("ISLR")
#install.packages("SmartEDA")
library("SmartEDA")

Carseats= ISLR::Carseats

q1 <- ExpCatViz(job, target=NULL, col ="slateblue4",clim=5,margin=1,Page = c(1,1),sample=3)
q1[[1]]
```
## Frequency Encoding 
```{r}
industry_freq <- job %>%
  group_by(industry) %>%
  summarise(industry_freq = n()) 

job <- job %>%
  left_join(industry_freq)

head(industry_freq)
```

# make factors 
```{r}
job %>%
  mutate_if(is.character,as.factor) %>%
  mutate(fraudulent = factor(fraudulent),
         employment_type = factor(employment_type), 
         telecommuting = factor(telecommuting),
         has_company_logo = factor(has_company_logo),
         has_questions = factor(has_questions),
         required_experience = factor(required_experience), 
         required_education = factor(required_education), 
         job_function = factor(job_function)) -> job

head(job)
```
```{r}
set.seed(123)

train_test_spit<- initial_split(job, prop = 0.7, strata = fraudulent)

train <- training(train_test_spit)
test  <- testing(train_test_spit)

sprintf("Train PCT : %1.2f%%", nrow(train)/ nrow(job) * 100)
sprintf("Test  PCT : %1.2f%%", nrow(test)/ nrow(job) * 100)


train_cv_folds <- vfold_cv(train, v=5)
sprintf("Kfold Count: %d", nrow(train_cv_folds))
```

```{r}
afinn <- get_sentiments("afinn")
sent <- job %>% 
  mutate(description = as.character(description)) %>%
  unnest_tokens(word, description) %>% 
  filter(!word %in% stop_words) %>% 
  inner_join(afinn) %>% 
  group_by(job_id) %>% 
  summarise(sentiment_title = sum(value))

job <- job %>% 
  left_join(sent) 

head(df,100)
```

```{r}
#Recipe
rf_recipe <- recipe(fraudulent ~  has_company_logo + description  + location , data = train) %>%
  step_unknown(location, has_company_logo) %>%
  step_novel(location, has_company_logo ) %>% 
  step_dummy(has_company_logo) %>%
  step_tokenize(location, description) %>%
  step_stopwords(location, description) %>%
  step_tokenfilter(location, description, min_times = 20) %>%
  step_tfidf(location, description) %>%
  step_meanimpute(all_numeric()) 
```

```{r}
# -- setup your tunning grid 
tune_grid <- grid_regular(trees(c(10,20)),
                          min_n(),
                          levels = 2)

print(tune_grid)

rf_model <- rand_forest(trees=tune(),
                        min_n=tune()) %>%
  set_engine("ranger", importance = "permutation") %>%
  set_mode("classification") 

rf_workflow <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(rf_model)


rf_tuning_results <- rf_workflow %>%
  tune_grid(
    resamples = train_cv_folds,
    grid = tune_grid
    )
```


```{r}
rf_tuning_results %>% 
  collect_metrics() %>%
  mutate_if(is.numeric, round,4)

rf_tuning_results %>%
  show_best("roc_auc") %>%
  print()

rf_best <- rf_tuning_results %>%
  select_best("roc_auc") 

print(rf_best)

rf_final_wf <- 
  rf_workflow %>% 
  finalize_workflow(rf_best)

print(rf_final_wf)

rf_final_fit  <- 
  rf_final_wf %>%
  fit(data = train) 
```
```{r}
options(yardstick.event_first = FALSE)
# model_name <- rf_workflow
  # -- training  
  predict(rf_final_fit , train, type="prob") %>%
    bind_cols(predict(rf_final_fit, train, type="class")) %>%
    bind_cols(.,train)-> scored_train 

  # -- testing 
  predict(rf_final_fit , test, type="prob") %>%
    bind_cols(predict(rf_final_fit, test, type="class")) %>%
    bind_cols(.,test) -> scored_test

  # -- AUC: Train and Test 
  scored_train %>% 
    metrics(fraudulent, .pred_1, estimate = .pred_class) %>%
    mutate(part="training") %>%
    bind_rows( scored_test %>% 
                 metrics(fraudulent, .pred_1, estimate = .pred_class) %>%
                 mutate(part="testing") ) %>%
    filter(.metric %in% c('accuracy','roc_auc')) %>%
    pivot_wider(names_from = .metric, values_from=.estimate)
  
  scored_train %>%
    conf_mat(fraudulent, .pred_class) %>%
    autoplot(type = "heatmap")
  
   scored_test %>%
    conf_mat(fraudulent, .pred_class) %>%
    autoplot(type = "heatmap")
  
  # -- ROC Charts 
  scored_train %>%
  mutate(model = "train") %>%
  bind_rows(scored_test %>%
              mutate(model="test")) %>%
  group_by(model) %>%
  roc_curve(fraudulent, .pred_1) %>%
  autoplot() 
  
  scored_test  %>%
  roc_curve(fraudulent, .pred_1) %>%
  mutate(fpr = round((1 - specificity),2),
         tpr = round(sensitivity,3),
         score_threshold =  1- round(.threshold,3)) %>%
  group_by(fpr) %>%
  summarise(score_threshold = max(score_threshold),
            tpr = max(tpr))%>%
  ungroup() %>%
  mutate(precision = tpr/(tpr + fpr)) %>%
  select(fpr, tpr, precision, score_threshold) %>%
  filter(fpr <= 0.1)

    # -- variable importance: top 10
  rf_final_fit %>%
    extract_fit_parsnip() %>%
  vip(num_features = 10)
  
  # score distribution
ggplot(scored_test,aes(.pred_1)) +
geom_histogram(data=subset(scored_test, fraudulent == 1), bins=100,fill = "red", alpha = 0.5) +
geom_histogram(data=subset(scored_test, fraudulent == 0), bins=100,fill = "blue", alpha = 0.5) +   
geom_vline(xintercept=0.8) +
labs(title="score distribution 1 = red, 0 = blue")
```

```{r}
# -- define recipe 
xgb_recipe <- recipe(fraudulent ~  has_company_logo + description  + location , data = train) %>%
  step_unknown(location, has_company_logo) %>%
  step_novel(location, has_company_logo ) %>% 
  step_dummy(has_company_logo) %>%
  step_tokenize(location, description) %>%
  step_stopwords(location, description) %>%
  step_tokenfilter(location, description, min_times = 20) %>%
  step_tfidf(location, description) %>%
  step_impute_mean(all_numeric()) 
```

```{r}
xgb_model <- boost_tree(
  trees = tune(), 
  tree_depth = tune(),       ## how deep of a tree, model complexity
  min_n = 2,            ## minimum number of observations 
  learn_rate = 0.5        ## step size
) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

xgb_model
```
```{r}
tune_grid <- grid_random(
    trees(c(10,300)),
    tree_depth(c(10,25)))
print(tune_grid)
```


```{r}
# -- setup workflow 
xgb_workflow <- workflow() %>%
  add_recipe(xgb_recipe) %>%
  add_model(xgb_model) 

# -- setup parallel process 
all_cores <- detectCores(logical = TRUE)
sprintf("# of Logical Cores: %d", all_cores)
cl <- makeCluster(all_cores)
registerDoParallel(cl)

# -- train!! K times for each parameter -- 
library(xgboost)
xgb_tuning_results <- xgb_workflow %>% 
  tune_grid(
    resamples = train_cv_folds,
    grid = tune_grid,
    control = control_resamples(save_pred = TRUE)
    )

xgb_tuning_results
```

```{r}
## -- results of tuning -- 
xgb_tuning_results %>% 
  collect_metrics() %>%
  mutate_if(is.numeric, round,4) %>% 
  pivot_wider(names_from = .metric, values_from=c(mean, std_err))
```

```{r}
## - visualize 
xgb_tuning_results %>%
  collect_metrics() %>%
  mutate_if(is.numeric, round,4) %>%
  ggplot(aes(tree_depth, mean, )) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) 
```

```{r}
xgb_tuning_results %>%
  show_best("roc_auc") %>%
  print()

xgb_best <- xgb_tuning_results %>%
  select_best("roc_auc") 

print(xgb_best)
```

```{r}
xgb_final_wf <- 
  xgb_workflow %>% 
  finalize_workflow(xgb_best)

print(xgb_final_wf)

xgb_final_fit  <- 
  xgb_final_wf %>%
  fit(data = train) 
```

```{r}
options(yardstick.event_first= FALSE)
predict(xgb_final_fit, train, type="prob") %>%
  bind_cols(predict(xgb_final_fit, train, type="class"))%>%
  bind_cols(train) -> train_scoredxgb

predict(xgb_final_fit, test, type="prob") %>%
  bind_cols(predict(xgb_final_fit, test, type="class")) %>%
  bind_cols(test) -> test_scoredxgb
```


# xgb model evaluation

```{r}
# -- AUC: Train and Test 
  train_scoredxgb %>% 
    metrics(fraudulent, .pred_1, estimate = .pred_class) %>%
    mutate(part="training") %>%
    bind_rows( test_scoredxgb %>% 
                 metrics(fraudulent, .pred_1, estimate = .pred_class) %>%
                 mutate(part="testing") ) %>%
    filter(.metric %in% c('accuracy','roc_auc')) %>%
    pivot_wider(names_from = .metric, values_from=.estimate)
  
  train_scoredxgb %>%
    conf_mat(fraudulent, .pred_class) %>%
    autoplot(type = "heatmap")
  
   test_scoredxgb %>%
    conf_mat(fraudulent, .pred_class) %>%
    autoplot(type = "heatmap")
  
  # -- ROC Charts 
  train_scoredxgb %>%
  mutate(model = "train") %>%
  bind_rows(test_scoredxgb %>%
              mutate(model="test")) %>%
  group_by(model) %>%
  roc_curve(fraudulent, .pred_1) %>%
  autoplot() 
  
  test_scoredxgb  %>%
  roc_curve(fraudulent, .pred_1) %>%
  mutate(fpr = round((1 - specificity),2),
         tpr = round(sensitivity,3),
         score_threshold =  1- round(.threshold,3)) %>%
  group_by(fpr) %>%
  summarise(score_threshold = max(score_threshold),
            tpr = max(tpr))%>%
  ungroup() %>%
  mutate(precision = tpr/(tpr + fpr)) %>%
  select(fpr, tpr, precision, score_threshold) %>%
  filter(fpr <= 0.5)
 
  # -- variable importance: top 10    
  xgb_final_fit %>% 
  pull_workflow_fit() %>% 
  vip(10) + 
  labs(title="Top 10 variable importance") 
  
# score distribution
ggplot(test_scoredxgb,aes(.pred_1)) +
geom_histogram(data=subset(test_scoredxgb, fraudulent == 1), bins=100,fill = "red", alpha = 0.5) +
geom_histogram(data=subset(test_scoredxgb, fraudulent == 0), bins=100,fill = "blue", alpha = 0.5) +   
geom_vline(xintercept=0.8) +
labs(title="score distribution 1 = red, 0 = blue")
```
# Random Forest model evaluation
```{r}
 # -- AUC: Train and Test 
  scored_train %>% 
    metrics(fraudulent, .pred_1, estimate = .pred_class) %>%
    mutate(part="training") %>%
    bind_rows( scored_test %>% 
                 metrics(fraudulent, .pred_1, estimate = .pred_class) %>%
                 mutate(part="testing") ) %>%
    filter(.metric %in% c('accuracy','roc_auc')) %>%
    pivot_wider(names_from = .metric, values_from=.estimate)
  
  scored_train %>%
    conf_mat(fraudulent, .pred_class) %>%
    autoplot(type = "heatmap")
  
   scored_test %>%
    conf_mat(fraudulent, .pred_class) %>%
    autoplot(type = "heatmap")

# -- variable importance: top 10   
rf_final_fit  %>% 
  extract_fit_parsnip() %>%
  vip(num_features = 10) +
  labs(title="Top 10 variable importance") 
   
 # -- ROC Charts 
  scored_train %>%
  mutate(model = "train") %>%
  bind_rows(scored_test %>%
              mutate(model="test")) %>%
  group_by(model) %>%
  roc_curve(fraudulent, .pred_1) %>%
  autoplot() 
  
  
  # -- operating range 
  scored_test  %>%
  roc_curve(fraudulent, .pred_1) %>%
  mutate(fpr = round((1 - specificity),2),
         tpr = round(sensitivity,3),
         score_threshold =  1- round(.threshold,3)) %>%
  group_by(fpr) %>%
  summarise(score_threshold = max(score_threshold),
            tpr = max(tpr))%>%
  ungroup() %>%
  mutate(precision = tpr/(tpr + fpr)) %>%
  select(fpr, tpr, precision, score_threshold) %>%
  filter(fpr <= 0.5)
  
# score distribution
ggplot(scored_test,aes(.pred_1)) +
geom_histogram(data=subset(scored_test, fraudulent == 1), bins=100,fill = "red", alpha = 0.5) +
geom_histogram(data=subset(scored_test, fraudulent == 0), bins=100,fill = "blue", alpha = 0.5) +   
geom_vline(xintercept=0.8) +
labs(title="score distribution 1 = red, 0 = blue")
```



